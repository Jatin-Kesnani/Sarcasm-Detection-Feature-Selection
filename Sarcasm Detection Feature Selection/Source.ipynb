{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 1\n",
    "\n",
    "!pip install stanfordnlp\n",
    "!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\n",
    "#Downloaded the zip file and extracted it manually\n",
    "\"\"\"\n",
    "!wget -q http://nlp.stanford.edu/software/stanford-corenlp-4.4.0.zip\n",
    "!unzip -q stanford-corenlp-4.4.0.zip\n",
    "\"\"\"\n",
    "\n",
    "#I was unable to start the CoreNLP server from python as it was giving error Unable to create sub process so executed line 14 directly on cmd\n",
    "\"\"\"\n",
    "Start the Stanford CoreNLP server\n",
    "import os\n",
    "os.chdir('stanford-corenlp-4.4.0')\n",
    "!java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 &\n",
    "\n",
    "Move back to the parent directory\n",
    "os.chdir('..')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 2\n",
    "\n",
    "!pip install pandas\n",
    "!pip install requests\n",
    "!pip install openpyxl\n",
    "!pip install Flask\n",
    "\n",
    "from openpyxl import Workbook\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 3\n",
    "\n",
    "#Preprocessing\n",
    "\n",
    "def lemmatizee(text):\n",
    "    url = 'http://localhost:9000'\n",
    "    properties = {\n",
    "        'annotators': 'tokenize,ssplit,pos,lemma',\n",
    "        'outputFormat': 'json'\n",
    "    }\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        params={'properties': json.dumps(properties)},\n",
    "        data=text.encode('utf-8'),\n",
    "        headers={'Content-Type': 'text/plain'}\n",
    "    )\n",
    "    lemmatized_text = ''\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        for sentence in data['sentences']:\n",
    "            for token in sentence['tokens']:\n",
    "                lemmatized_text += token['lemma'] + ' '\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code)\n",
    "\n",
    "    return lemmatized_text.strip()\n",
    "data_list = []\n",
    "with open('Sarcasm_Headlines_Dataset.json', 'r') as file:\n",
    "  for line in file:\n",
    "    data = json.loads(line)\n",
    "    data_list.append(data)\n",
    "\n",
    "Converted_Dataset = pd.DataFrame(data_list)\n",
    "Converted_Dataset = Converted_Dataset[['headline', 'is_sarcastic']]\n",
    "for i,x in Converted_Dataset.iterrows():\n",
    "    x['headline'] = lemmatizee(x['headline'])\n",
    "\n",
    "Converted_Dataset.to_excel('lemmatizedDataset.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 4\n",
    "\n",
    "#Module 1\n",
    "import json\n",
    "import requests\n",
    "\n",
    "endpoint = 'http://api.conceptnet.io/c/en/'\n",
    "params = {\n",
    "    'filter': 'core',\n",
    "    'limit': 1000\n",
    "}\n",
    "def conceptNet(word):\n",
    "    response = requests.get(endpoint + word, params=params)\n",
    "    data = json.loads(response.text)\n",
    "    edges = data['edges']\n",
    "    edges.sort(key=lambda x: x['weight'], reverse=True)\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 5\n",
    "\n",
    "!pip install senticnet\n",
    "!pip install sentistrength\n",
    "\n",
    "from senticnet.senticnet import SenticNet\n",
    "from sentistrength import PySentiStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 6\n",
    "\n",
    "#Module 2\n",
    "sn = SenticNet()\n",
    "def senticNetScore(word):\n",
    "    try:\n",
    "        polarity = sn.polarity_value(word)\n",
    "        return float(polarity) * 5\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "senti = PySentiStr()\n",
    "\n",
    "senti.setSentiStrengthPath('SentiStrength.jar')\n",
    "senti.setSentiStrengthLanguageFolderPath('SentiStrength_Data')\n",
    "def sentiStrengthScore(word):\n",
    "    result = senti.getSentiment(word)\n",
    "    return result\n",
    "\n",
    "def sWordScore(word):\n",
    "    sNet = senticNetScore(word)\n",
    "    sStrength = sentiStrengthScore(word)[0]\n",
    "    if sNet == None and sStrength == 0:\n",
    "        expansion = conceptNet(word)\n",
    "        if len(expansion) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            score = 0\n",
    "            expansion = expansion[:5]\n",
    "            for edge in expansion:\n",
    "                try:\n",
    "                    score += senticNetScore(edge['end']['label'])\n",
    "                except:\n",
    "                    score += 0\n",
    "            return score / 5\n",
    "    elif sNet == None:\n",
    "        return sStrength\n",
    "    elif sStrength == 0:\n",
    "        return sNet\n",
    "    else:\n",
    "        return (sNet + sStrength) / 2\n",
    "\n",
    "def posNegScore(sentenceScores):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for i in sentenceScores:\n",
    "        if (i > 0):\n",
    "            pos += i\n",
    "        elif (i < 0):\n",
    "            neg += i\n",
    "    return pos,neg\n",
    "\n",
    "def isContradiction(sentence):\n",
    "    print(\"Sentence: \",sentence)\n",
    "    results = []\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        results.append(sWordScore(word))\n",
    "    posSum, negSum = posNegScore(results)\n",
    "    if posSum != 0 and negSum != 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 7\n",
    "\n",
    "!pip install space\n",
    "!pip install nltk\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 8\n",
    "\n",
    "#Module 3\n",
    "\n",
    "spacyNLP = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def extractSubject(sentence):\n",
    "    doc = spacyNLP(sentence)\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            return token.text\n",
    "    return None\n",
    "\n",
    "def hasAntecedents(text):\n",
    "    doc = spacyNLP(text)\n",
    "    antecedents = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
    "            for mention in doc.ents:\n",
    "                if mention.start <= token.i < mention.end:\n",
    "                    antecedents.append(mention.text)\n",
    "    return len(antecedents) > 0\n",
    "\n",
    "def identicalPronouns(w1, w2):\n",
    "    return lemmatizer.lemmatize(w1, 'n') == lemmatizer.lemmatize(w2, 'n')\n",
    "\n",
    "def identicalSubjects(w1, w2):\n",
    "    cleanedSubject1 = re.sub(r'[^a-zA-Z]', '', w1)\n",
    "    cleanedSubject2 = re.sub(r'[^a-zA-Z]', '', w2)\n",
    "    return cleanedSubject1 == cleanedSubject2\n",
    "\n",
    "def definiteNounPhraseFeature(text, w2):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == w2 and i > 0 and tokens[i - 1] == 'the':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def demonstrativeNounPhraseFeature(text, w2):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == w2 and i > 0 and tokens[i - 1] in ['this', 'that', 'these', 'those']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def properNameFeature(w1, w2):\n",
    "    taggedWords = nltk.pos_tag([w1, w2])\n",
    "    return all(tag in ['NNP', 'NNPS'] for word, tag in taggedWords)\n",
    "\n",
    "def checkCoherence(sentence):\n",
    "    sentences = nltk.sent_tokenize(sentence)\n",
    "    if len(sentences) > 1:\n",
    "        if hasAntecedents(sentence):\n",
    "            return True\n",
    "        subject1 = extractSubject(sentences[0])\n",
    "        subject2 = extractSubject(sentences[1])\n",
    "        if identicalPronouns(subject1, subject2) or identicalSubjects(subject1, subject2) or \\\n",
    "           definiteNounPhraseFeature(sentences[1], subject2) or demonstrativeNounPhraseFeature(sentences[1], subject2) or \\\n",
    "           properNameFeature(subject1, subject2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 9\n",
    "\n",
    "#Module 4\n",
    "import re\n",
    "\n",
    "def countEmoticons(headline):\n",
    "    return len(re.findall(r'[^\\w\\s,]', headline))\n",
    "\n",
    "def countRepetitivePunctuations(headline):\n",
    "    return len(re.findall(r'([\\W_]){2,}', headline))\n",
    "\n",
    "def countRepetitiveSequences(headline):\n",
    "    return len(re.findall(r'(\\S)\\1{1,}', headline))\n",
    "\n",
    "def countCapitalLetters(headline):\n",
    "    return len(re.findall(r'[A-Z]', headline))\n",
    "\n",
    "def countExclamationMarks(headline):\n",
    "    return len(re.findall(r'!', headline))\n",
    "\n",
    "with open(\"Slang_and_Booster_Words.txt\", 'r') as file:\n",
    "    slang_booster = [line.strip().casefold() for line in file if line.strip()]\n",
    "\n",
    "print(slang_booster)\n",
    "\n",
    "def countBoostersAndSlangs(headline):\n",
    "    return sum(1 for word in headline.split() if word.lower() in slang_booster)\n",
    "\n",
    "with open(\"Idioms.txt\", 'r') as file:\n",
    "    idioms = [line.strip().casefold() for line in file if line.strip()]\n",
    "\n",
    "print(idioms)\n",
    "\n",
    "def countIdioms(headline):\n",
    "    return sum(1 for word in headline.split() if word.lower() in idioms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 10\n",
    "\n",
    "#FeatureSet Definition & Computation\n",
    "\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "features = {\"headline\":[], \"is_sarcastic\": [], \"contra\" : [], \"contra_plus_coher\" : [],\n",
    "            \"pos_low\": [], \"pos_med\" : [], \"pos_high\" : [],\n",
    "            \"neg_low\": [], \"neg_med\" : [], \"neg_high\" : [],\n",
    "            \"emo_low\": [], \"emo_med\" : [], \"emo_high\" : [],\n",
    "            \"punct_low\": [], \"punct_med\" : [], \"punct_high\" : [],\n",
    "            \"char_low\": [], \"char_med\" : [], \"char_high\" : [],\n",
    "            \"cap_low\": [], \"cap_med\" : [], \"cap_high\" : [],\n",
    "            \"slangBooster_low\": [], \"slangBooster_med\" : [], \"slangBooster_high\" : [],\n",
    "            \"exclaim_low\": [], \"exclaim_med\" : [], \"exclaim_high\" : [],\n",
    "            \"idiom_low\": [], \"idiom_med\" : [], \"idiom_high\" : []}\n",
    "\n",
    "def calLowMedHigh(score,a,b,t):\n",
    "    if score < a and t == \"low\":\n",
    "        return 1\n",
    "    elif score >= a and score <= b and t == \"med\":\n",
    "        return 1\n",
    "    elif score > b and t == \"high\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculateFeatures(headline, isSarcastic = None):\n",
    "    vector = copy.deepcopy(features)\n",
    "    for i in vector:\n",
    "        vector[i] = [0]\n",
    "    vector[\"headline\"] = headline\n",
    "    if isSarcastic == None:\n",
    "        vector[\"is_sarcastic\"] = [None]\n",
    "    else:\n",
    "        vector[\"is_sarcastic\"] = [isSarcastic]\n",
    "    sen = re.sub(r'[^\\w\\s]|[\\d]', '', headline)\n",
    "    \n",
    "    words = nltk.word_tokenize(sen)\n",
    "    if len(nltk.sent_tokenize(headline)) > 1:\n",
    "        if isContradiction(sen) and checkCoherence(sen):\n",
    "            vector[\"contra_plus_coher\"] = [1]\n",
    "        else:\n",
    "            vector[\"contra_plus_coher\"] = [0]\n",
    "    else:\n",
    "        if isContradiction(sen):\n",
    "            vector[\"contra\"] = [1]\n",
    "        else:\n",
    "            vector[\"contra\"] = [0] \n",
    "    sample = []\n",
    "    for i in words:\n",
    "        sample.append(sWordScore(i))\n",
    "    print(\"Calculating Positive Negative Scores for\", headline)\n",
    "    posSum, negSum = posNegScore(sample)\n",
    "    print(\"Calculating Emoticons Scores for\", headline)\n",
    "    emoSum = countEmoticons(headline)\n",
    "    print(\"Calculating Repetitive Punctuations Scores for\", headline)\n",
    "    punctSum = countRepititivePunctuations(headline)\n",
    "    print(\"Calculating Repetitive Sequence Scores for\", headline)\n",
    "    charSum = countRepititiveSequences(headline)\n",
    "    print(\"Calculating Capital Letters in\", headline)\n",
    "    capSum = countCapitalLetters(headline)\n",
    "    print(\"Calculating Slangs and Boosters in\", headline)\n",
    "    bSSum = countBoostersAndSlangs(headline)\n",
    "    print(\"Calculating Exclaimation Marks in\", headline)\n",
    "    exclaimSum = countExclamationMarks(headline)\n",
    "    print(\"Calculating Idioms in\", headline)\n",
    "    idiomSum = countIdioms(headline)\n",
    "\n",
    "    vector[\"pos_low\"] = [calLowMedHigh(posSum,0,1,\"low\")]\n",
    "    vector[\"pos_med\"] = [calLowMedHigh(posSum,0,1,\"med\")]\n",
    "    vector[\"pos_high\"] = [calLowMedHigh(posSum,0,1,\"high\")]\n",
    "    vector[\"neg_low\"] = [calLowMedHigh(negSum,-1,0,\"high\")]\n",
    "    vector[\"neg_med\"] = [calLowMedHigh(negSum,-1,0,\"med\")]\n",
    "    vector[\"neg_high\"] = [calLowMedHigh(negSum,-1,0,\"low\")]\n",
    "\n",
    "    vector[\"emo_low\"] = [calLowMedHigh(emoSum,1,3,\"low\")]\n",
    "    vector[\"emo_med\"] = [calLowMedHigh(emoSum,1,3,\"med\")]\n",
    "    vector[\"emo_high\"] = [calLowMedHigh(emoSum,1,3,\"high\")]\n",
    "    vector[\"punct_low\"] = [calLowMedHigh(punctSum,1,3,\"low\")]\n",
    "    vector[\"punct_med\"] = [calLowMedHigh(punctSum,1,3,\"med\")]\n",
    "    vector[\"punct_high\"] = [calLowMedHigh(punctSum,1,3,\"high\")]\n",
    "    vector[\"char_low\"] = [calLowMedHigh(charSum,1,3,\"low\")]\n",
    "    vector[\"char_med\"] = [calLowMedHigh(charSum,1,3,\"med\")]\n",
    "    vector[\"char_high\"] = [calLowMedHigh(charSum,1,3,\"high\")]\n",
    "    vector[\"cap_low\"] = [calLowMedHigh(capSum,1,3,\"low\")]\n",
    "    vector[\"cap_med\"] = [calLowMedHigh(capSum,1,3,\"med\")]\n",
    "    vector[\"cap_high\"] = [calLowMedHigh(capSum,1,3,\"high\")]\n",
    "    vector[\"slangBooster_low\"] = [calLowMedHigh(bSSum,1,3,\"low\")]\n",
    "    vector[\"slangBooster_med\"] = [calLowMedHigh(bSSum,1,3,\"med\")]\n",
    "    vector[\"slangBooster_high\"] = [calLowMedHigh(bSSum,1,3,\"high\")]\n",
    "    vector[\"exclaim_low\"] = [calLowMedHigh(exclaimSum,1,3,\"low\")]\n",
    "    vector[\"exclaim_med\"] = [calLowMedHigh(exclaimSum,1,3,\"med\")]\n",
    "    vector[\"exclaim_high\"] = [calLowMedHigh(exclaimSum,1,3,\"high\")]\n",
    "    vector[\"idiom_low\"] = [calLowMedHigh(idiomSum,1,3,\"low\")]\n",
    "    vector[\"idiom_med\"] = [calLowMedHigh(idiomSum,1,3,\"med\")]\n",
    "    vector[\"idiom_high\"] = [calLowMedHigh(idiomSum,1,3,\"high\")]\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 11\n",
    "\n",
    "#Feature Set Computation over Lemmatized DataSet\n",
    "\n",
    "#It took more than 76 hours for the dataset of 26710 tweets\n",
    "df1 = pd.read_excel(\"lemmatizedDataset.xlsx\")\n",
    "final = pd.DataFrame(features)\n",
    "\n",
    "for i in range(len(df1[\"headline\"])):\n",
    "    x = pd.DataFrame(calculateFeatures(df1[\"headline\"][i], df1[\"is_sarcastic\"][i]))\n",
    "    final = pd.concat([final, x], ignore_index=True)\n",
    "print(final)\n",
    "final.to_excel('FeatureSet.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 12\n",
    "\n",
    "!pip install scikit-learn\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 13\n",
    "\n",
    "#Evaluations Metrix Definitions\n",
    "\n",
    "final = pd.read_excel(\"FeatureSet.xlsx\")\n",
    "results = pd.DataFrame({\"Method\" : [], \"Precision\" : [], \"Recall\": [], \"F-Measure\": [], \"Accuracy\": []})\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f_measure = f1_score(y_true, y_pred, average='binary')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return precision, recall, f_measure, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 14\n",
    "\n",
    "#Contradiction in Sentiment Scores\n",
    "\n",
    "y = final[\"is_sarcastic\"]\n",
    "contradiction = []\n",
    "\n",
    "for i in range(len(final[\"headline\"])):\n",
    "    if(final['contra'][i] == 1 or final[\"contra_plus_coher\"][i] == 1):\n",
    "        contradiction.append(1)\n",
    "    else:\n",
    "        contradiction.append(0)\n",
    "\n",
    "contraPre, contraRecall, contraF, contraAccuracy = calculate_metrics(y, contradiction)\n",
    "print(\"Contradiction in Sentiment Scores\")\n",
    "print(contraPre)\n",
    "print(contraRecall)\n",
    "print(contraF)\n",
    "print(contraAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 15\n",
    "\n",
    "#N-Grams Prediction\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "ngramX = final[\"headline\"]\n",
    "ngramRange = (1,3)\n",
    "vectorizer = CountVectorizer(ngram_range=ngramRange)\n",
    "vectorizedX = vectorizer.fit_transform(ngramX)\n",
    "ngramSVM = SVC(kernel='linear')\n",
    "ngramSVM.fit(vectorizedX, y)\n",
    "ngramPred = cross_val_predict(ngramSVM, vectorizedX, y, cv=10, n_jobs=4)\n",
    "\n",
    "ngramPre, ngramRecall, ngramF, ngramAccuracy = calculate_metrics(y, ngramPred)\n",
    "print(\"Feature Space Classification\")\n",
    "print(ngramPre)\n",
    "print(ngramRecall)\n",
    "print(ngramF)\n",
    "print(ngramAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 16\n",
    "\n",
    "#Feature Space Prediction\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "featuresX = final.drop('is_sarcastic', axis = 1)  # Features\n",
    "featuresX = featuresX.drop('headline', axis = 1)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizedX = vectorizer.fit_transform(featuresX)\n",
    "featuresSVM = SVC(kernel='linear')\n",
    "assert len(featuresX) == len(y), \"Mismatched lengths between features and labels\"\n",
    "featuresSVM.fit(featuresX, y)\n",
    "featuresPred = cross_val_predict(featuresSVM, featuresX, y, cv=10,n_jobs=4)\n",
    "\n",
    "featuresPre, featuresRecall, featuresF, featuresAccuracy = calculate_metrics(y, featuresPred)\n",
    "print(\"Feature Space Classification\")\n",
    "print(featuresPre)\n",
    "print(featuresRecall)\n",
    "print(featuresF)\n",
    "print(featuresAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 17\n",
    "\n",
    "#N-grams + Feature Set Combined Prediction\n",
    "\n",
    "ngramFeatures = []\n",
    "for i in range(len(ngramPred)):\n",
    "    if ngramPred[i] == featuresPred[i]:\n",
    "        ngramFeatures.append(ngramPred[i])\n",
    "    else:\n",
    "        ngramsMargin = ngramSVM.decision_function(vectorizedX[i]) \n",
    "        featuresMargin = featuresSVM.decision_function([featuresX.iloc[i]])  # Remove the wrapping of [featureX[i]] in square brackets\n",
    "        if abs(ngramsMargin) > abs(featuresMargin):\n",
    "            ngramFeatures.append(ngramPred[i])\n",
    "        else:\n",
    "            ngramFeatures.append(featuresPred[i]) \n",
    "            \n",
    "combinedPre, combinedRecall, combinedF, combinedAccuracy = calculate_metrics(y, ngramFeatures)\n",
    "print(\"Combined N-grams & Feature Set Classification\")\n",
    "print(combinedPre)\n",
    "print(combinedRecall)\n",
    "print(combinedF)\n",
    "print(combinedAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CelL No. 18\n",
    "\n",
    "#Saving Evaluation Metrics\n",
    "\n",
    "res = {\"Method\" : [], \"Precision\" : [], \"Recall\": [], \"F-Measure\": [], \"Accuracy\": []}\n",
    "res[\"Method\"].append(\"contradiction in Sentiment Scores\")\n",
    "res[\"Precision\"].append(contraPre)\n",
    "res[\"Recall\"].append(contraRecall)\n",
    "res[\"F-Measure\"].append(contraF)\n",
    "res[\"Accuracy\"].append(contraAccuracy)\n",
    "\n",
    "res[\"Method\"].append(\"N-grams SVC Classification\")\n",
    "res[\"Precision\"].append(ngramPre)\n",
    "res[\"Recall\"].append(ngramRecall)\n",
    "res[\"F-Measure\"].append(ngramF)\n",
    "res[\"Accuracy\"].append(ngramAccuracy)\n",
    "\n",
    "res[\"Method\"].append(\"Feature-Space SVC Classification\")\n",
    "res[\"Precision\"].append(featuresPre)\n",
    "res[\"Recall\"].append(featuresRecall)\n",
    "res[\"F-Measure\"].append(featuresF)\n",
    "res[\"Accuracy\"].append(featuresAccuracy)\n",
    "\n",
    "res[\"Method\"].append(\"Combined N-grams & Feature Space\")\n",
    "res[\"Precision\"].append(combinedPre)\n",
    "res[\"Recall\"].append(combinedRecall)\n",
    "res[\"F-Measure\"].append(combinedF)\n",
    "res[\"Accuracy\"].append(combinedAccuracy)\n",
    "\n",
    "results = pd.DataFrame(res)\n",
    "results.to_excel(\"Evaluation.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
